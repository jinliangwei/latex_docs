\documentclass[12pt, a4paper]{llncs}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{mathpazo} % Loading the package uses font palatino
\usepackage{wrapfig}
\usepackage{authblk}

\usepackage{style/alex}

\begin{document}
\title{15-740 Literature Review}
\author{Grant Skudlaret, Jinliang Wei}
\institute{}

\maketitle

There have been serveral previous works that explored dynamic cache partitioning
to improve the scalability of the multi-core processors. 
\cite{Qureshi:2006:UCP:1194816.1194855} proposed a utility-based cache 
partitioning strategy. In their approach, each core is associated with a utility
monitor (UMON). UMON has a tag directory which caches a tag per set per way. 
In order to reduce hardware overhead, UMON may cache only one tag for the same 
cache way in all sets. By using UMON, a cache miss line can be obtained, which 
depicts the relationship between number of cache misss and the number of ways 
assigned to this core. By greedily assigning each cache way to the core where 
the cache utility can be maximized (reducing most cache misses), this approach
partitions the cache to maximize utility (minimize cache misses). This is called
look ahead algorithm.

\cite{Qureshi:2006:CMC:1150019.1136501} pointed out that cache misses are not 
equivalently expensive. Parallel misses are much cheaper than isolated misses 
because they can be served in parallel. Instead of minimizing the number of 
cache misses, a better cache partitioning strategy might be minimizing MLP-based
cache cost. This strategy was explored by \cite{conf/IEEEpact/MoretoCRV07} which
demonstrated MLP-aware cache partitioning indeed achieves better performance.
Recently, \cite{conf/IEEEpact/BeckmannS13} studied partitioning fully-associated
cache to data and it reduced the cost of look-ahead algorithm by peek-ahead 
algorithm.

All previous works predict future cache usage based on previous cache miss 
curve. This works if the cache usage pattern remains similar across different 
phases of the program. When the memory usage pattern of the program changes 
across phases, the prediction might be inaccurate. In our project, we'd like to
further improve cache partitioning based on program hints for future memory 
usage given by users, OS, or compiler.

\cite{Ipek:2008:SMC:1381306.1382172} describes a novel memory controller design
 which would use adaptive 
scheduling based on machine learning. Using reinforcement learning, their 
scheduler would optimize scheduling on the fly. Controller-state action pairs 
are assigned reward values, and when commands are issued, the controller tries 
to choose the command with the greatest long term value. A learning controller 
brings about some great benefits to program performance. Primary amongst these 
is that the controller optimize for bus bandwidth, and does so on the fly. Many 
scheduling algorithms attempt provide the best bandwidth in the general case, 
though there often weak spots in their approaches which reduce memory 
throughput. By learning and being adaptable, the authors’ scheme fights this 
weakness. In addition, the rewards system takes the core where the memory 
request originated from, allowing the scheme to fight against starvation as 
well. However the capabilities of the scheduling system are limited. While in 
theory, the machine learning algorithm they chose should be able to take into 
account infinitely many states and inputs, hardware and computation time hampers
 the scheduling optimization possibilities. While the authors optimized their 
algorithm for the resources they had available, there are various scenarios 
which they were not able to account for due to hardware limitations, creating 
weak spots in their system.

\cite{10.1109/MM.2008.48} proposes a thread scheduling scheme to minimize LLC contention based 
on architectural observations made by the OS at runtime. By leveraging features 
of the chosen architecture, the authors were able to track cache hit/miss ratios
 as well as absolute counts of hits and misses on caches in the system. Using 
these metrics, threads running in the system were assigned weights which 
corresponded to their cache usage. Threads were then scheduled on the cores in 
such a way that these weights were spread as evenly as possible across shared 
caches. The benefits to this approach are easily tangible. Scheduling threads 
in this way ensures that memory intensive processes are given an appropriate 
amount of resources rather than being forced to share an unnecessary and counter
productive portion of cache. Additionally, from an overall system perspective, 
the scheme greatly assists in balancing load on the system.  However, while the 
observation overhead on the system was low, the scheme’s observation granularity
 is limited. In their scheme, the author’s only use data from the immediately 
past phase of a program to predict future weights. With more detailed 
observations about program behavior, thread weight prediction could be made more
 accurate for any given moment during a run of the program. 

\bibliographystyle{plain}
\bibliography{reference}

\end{document}